# -*- coding: utf-8 -*-
"""Group9_Heart_Disease_Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tI4wvRN3dLMfpxo7oapxcyGBs3YbifDQ
"""

!pip install tensorflow matplotlib

import scipy.signal as signal  # Make sure this import is included

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import os
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# Path to your dataset in Google Drive
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# List of subfolder names corresponding to the 5 conditions
categories = ['AS', 'MR', 'MS', 'MVP', 'N']

# Function to load audio files with a sample rate of 8000 Hz
def load_audio_from_drive(file_name, folder):
    file_path = os.path.join(dataset_folder, folder, file_name)
    audio, sr = librosa.load(file_path, sr=8000)
    return audio, sr

# Function to compute Mel spectrogram
def compute_spectrogram(audio, sr):
    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=4000)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
    return mel_spectrogram_db

# Function to pad spectrogram to fixed width
def pad_spectrogram(spectrogram, target_width):
    pad_width = target_width - spectrogram.shape[1]
    if pad_width > 0:
        spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode='constant')
    return spectrogram

# Plotting function with axis labels
def plot_spectrogram(spectrogram, sr=8000):
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(spectrogram,
                             x_axis='time',
                             y_axis='mel',
                             sr=sr,
                             fmax=4000,
                             cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Mel-frequency Spectrogram')
    plt.xlabel('Time')
    plt.ylabel('Hz')
    plt.tight_layout()
    plt.show()

# Initialize storage for spectrograms and labels
spectrograms = []
labels = []
max_width = 0

# Loop through each heart sound category
for category in categories:
    print(f"Processing category: {category}")
    audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]

    for audio_file in audio_files:
        audio, sr = load_audio_from_drive(audio_file, category)
        spectrogram = compute_spectrogram(audio, sr)

        # Track maximum width for padding later
        max_width = max(max_width, spectrogram.shape[1])

        spectrograms.append(spectrogram)
        labels.append(categories.index(category))

# Pad all spectrograms to same width
padded_spectrograms = [pad_spectrogram(s, max_width) for s in spectrograms]

# Convert to NumPy arrays
padded_spectrograms = np.array(padded_spectrograms)
labels = np.array(labels)

print(f"Padded Spectrograms shape: {padded_spectrograms.shape}")
print(f"Labels shape: {labels.shape}")

# Display a sample spectrogram
if len(padded_spectrograms) > 0:
    plot_spectrogram(padded_spectrograms[0], sr=8000)
else:
    print("No spectrograms to display.")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import os
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# Path to your dataset in Google Drive
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# List of subfolder names corresponding to the 5 conditions
categories = ['AS', 'MR', 'MS', 'MVP', 'N']

# Function to load audio files with a sample rate of 8000 Hz
def load_audio_from_drive(file_name, folder):
    file_path = os.path.join(dataset_folder, folder, file_name)
    audio, sr = librosa.load(file_path, sr=8000)  # Load with 8000 Hz as per the paper
    return audio, sr

# Function to compute Mel spectrogram (using 8000 Hz max frequency)
def compute_spectrogram(audio, sr):
    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=4000)  # fmax is typically up to 4000 Hz for heart sounds
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
    return mel_spectrogram_db

# Function to plot the Mel spectrogram for visual inspection
def plot_spectrograms_for_category(category, audio_files):
    plt.figure(figsize=(15, 8))
    for i, audio_file in enumerate(audio_files[:5]):  # Show the first 5 spectrograms for each category
        # Load and process each audio file
        audio, sr = load_audio_from_drive(audio_file, category)
        spectrogram = compute_spectrogram(audio, sr)

        plt.subplot(2, 3, i+1)  # Plot in a 2x3 grid
        librosa.display.specshow(spectrogram, x_axis='time', y_axis='mel', cmap='viridis')
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'{category} - Sample {i+1}')

    plt.tight_layout()
    plt.show()

# Process and visualize spectrograms for each category
for category in categories:
    print(f"Processing and displaying spectrograms for category: {category}")

    # List all audio files in the current category folder
    audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]  # Filter for .wav files

    # Plot spectrograms for the category
    plot_spectrograms_for_category(category, audio_files)

# Define the path to each condition's folder (make sure to adjust paths accordingly)
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# List of conditions (subfolders)
categories = ['AS', 'MR', 'MS', 'MVP', 'N']
labels = ['Aortic Stenosis', 'Mitral Regurgitation', 'Mitral Stenosis', 'Mitral Valve Prolapse', 'Normal']

# Function to plot time-domain graph of a PCG signal
def plot_time_domain_signal(audio, title, sr=8000):
    # Generate time vector in seconds
    time = np.linspace(0, len(audio) / sr, num=len(audio))

    # Plot the time-domain signal
    plt.plot(time, audio)
    plt.title(title)
    plt.xlabel('Time [seconds]')
    plt.ylabel('Amplitude')
    plt.grid(True)

# Plot graphs for each condition
plt.figure(figsize=(15, 10))

for idx, category in enumerate(categories):
    # Load the first audio file for each category
    audio_file = os.listdir(os.path.join(dataset_folder, category))[0]  # First file in the folder
    audio, sr = librosa.load(os.path.join(dataset_folder, category, audio_file), sr=8000)

    # Plot each PCG signal
    plt.subplot(2, 3, idx + 1)  # Plot in a 2x3 grid
    plot_time_domain_signal(audio, f'({chr(97+idx)}) {labels[idx]} PCG Signal', sr=sr)

plt.tight_layout()
plt.show()

from IPython.display import Audio

# Path to your dataset in Google Drive
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# List of subfolder names corresponding to the 5 conditions
categories = ['AS', 'MR', 'MS', 'MVP', 'N']

# Function to load and return the audio data from the first file in each folder
def play_audio_from_folder(folder):
    audio_files = [f for f in os.listdir(os.path.join(dataset_folder, folder)) if f.endswith('.wav')]  # Filter for .wav files
    if audio_files:
        # Load the first audio file
        file_path = os.path.join(dataset_folder, folder, audio_files[0])
        return file_path
    else:
        return None

# Play and display the first audio file from each subfolder with appropriate headings
for category in categories:
    print(f"\n\n### {category} (Condition: {category}) ###")
    audio_file_path = play_audio_from_folder(category)

    if audio_file_path:
        display(Audio(audio_file_path))
    else:
        print(f"No audio files found in the {category} folder.")

"""## AUGMENTATION
 (jangan di run lagi)
"""

import numpy as np
import librosa
import librosa.display
import os
import matplotlib.pyplot as plt
from scipy.io.wavfile import write

# Path to your dataset in Google Drive
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# List of subfolder names corresponding to the 5 conditions
categories = ['AS', 'MR', 'MS', 'MVP', 'N']

# Background Deformation Augmentation Function
def background_deformation(audio, deformation_lambda=1000):
    """
    Apply background deformation (noise) to the audio signal.
    Args:
        audio (numpy.ndarray): Original audio signal.
        deformation_lambda (float): Control parameter for the deformation noise.
    Returns:
        numpy.ndarray: Augmented audio signal.
    """
    # Generate a random background deformation signal β
    deformation = np.random.uniform(0, 1, size=audio.shape)  # Random noise in the range (0, 1)

    # Apply the deformation according to the formula α = I + β * λ
    augmented_audio = audio + deformation * deformation_lambda

    # Clip the values to ensure they stay within the valid range of audio signal
    augmented_audio = np.clip(augmented_audio, -1.0, 1.0)

    return augmented_audio

# Function to save augmented audio to a file
def save_augmented_audio(audio, output_path):
    """
    Save the augmented audio to a .wav file.
    Args:
        audio (numpy.ndarray): The augmented audio signal.
        output_path (str): The file path where the augmented audio will be saved.
    """
    # Normalize audio to 16-bit PCM format before saving
    audio_normalized = np.int16(audio * 32767)
    write(output_path, 8000, audio_normalized)

# Process and augment the dataset
def augment_dataset():
    augmented_data = []

    for category in categories:
        print(f"Processing and augmenting data for category: {category}")

        # List all audio files in the current category folder
        audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]  # Filter for .wav files

        for audio_file in audio_files:
            # Load the original audio file
            file_path = os.path.join(dataset_folder, category, audio_file)
            audio, sr = librosa.load(file_path, sr=8000)  # Load with 8000 Hz as per the paper

            # Apply background deformation (augmentation)
            augmented_audio = background_deformation(audio, deformation_lambda=1000)

            # Save the augmented audio to a new file in the same directory
            augmented_file_path = os.path.join(dataset_folder, category, f"augmented_{audio_file}")
            save_augmented_audio(augmented_audio, augmented_file_path)

            # Store the augmented data for future use
            augmented_data.append((augmented_audio, category))  # Can store it for future training

    return augmented_data

# Augment the dataset
augmented_data = augment_dataset()

# Show one example augmented spectrogram for visual inspection (first audio file from a category)
def show_augmented_spectrogram(augmented_audio):
    # Compute the Mel spectrogram
    mel_spectrogram = librosa.feature.melspectrogram(y=augmented_audio, sr=8000, n_mels=128, fmax=4000)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)

    # Plot the spectrogram
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Augmented Mel-frequency spectrogram')
    plt.show()

# Show the spectrogram of the first augmented audio file (example)
if augmented_data:
    show_augmented_spectrogram(augmented_data[0][0])  # Show the first augmented spectrogram

"""## PREPROCESSING
 (jangan di run lagi)

"""

def normalize_audio(audio):
    """
    Normalize the audio signal to fit within the 16-bit range.
    Args:
        audio (numpy.ndarray): Audio signal.
    Returns:
        numpy.ndarray: Normalized audio signal.
    """
    # Normalize to the range [-1, 1]
    audio = audio / np.max(np.abs(audio))

    # Convert to floating-point representation (16-bit normalized audio should be in range [-1, 1])
    audio_normalized = audio.astype(np.float32)
    return audio_normalized

def butterworth_filter(audio, lowcut=20.0, highcut=150.0, fs=8000, order=4):
    """
    Apply a Gaussian Butterworth filter to the audio signal to remove high-frequency noise.
    Args:
        audio (numpy.ndarray): Audio signal.
        lowcut (float): Low cutoff frequency in Hz (default 20 Hz).
        highcut (float): High cutoff frequency in Hz (default 150 Hz).
        fs (int): Sample rate (default 8000 Hz).
        order (int): Order of the filter (default 4).
    Returns:
        numpy.ndarray: Filtered audio signal.
    """
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist

    # Create Butterworth filter
    b, a = signal.butter(order, [low, high], btype='band')
    filtered_audio = signal.filtfilt(b, a, audio.astype(np.float32))  # Ensure it's float32

    return filtered_audio

def resample_audio(audio, target_length=20000, fs=8000):
    """
    Resample the audio signal to a target length (20,000 samples) at a new sample rate.
    Args:
        audio (numpy.ndarray): Audio signal.
        target_length (int): Target length for the audio in samples (default 20,000).
        fs (int): Sample rate of the original signal (default 8000 Hz).
    Returns:
        numpy.ndarray: Resampled audio signal.
    """
    # Resample the audio signal to match the target length
    duration = len(audio) / fs
    target_fs = target_length / duration  # Target sample rate
    resampled_audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)

    # Pad or truncate the signal to match the target length
    if len(resampled_audio) < target_length:
        resampled_audio = np.pad(resampled_audio, (0, target_length - len(resampled_audio)), 'constant')
    else:
        resampled_audio = resampled_audio[:target_length]

    return resampled_audio

def preprocess_pcg_audio(file_path, fs=8000, target_length=20000):
    """
    Preprocess the PCG audio signal: normalize, filter, and resample.
    Args:
        file_path (str): Path to the audio file.
        fs (int): Sample rate of the audio signal (default 8000 Hz).
        target_length (int): Target length for resampling (default 20,000).
    Returns:
        numpy.ndarray: Preprocessed audio signal.
    """
    # Load the audio file
    audio, _ = librosa.load(file_path, sr=fs)

    # Apply Butterworth filter to remove high-frequency noise
    filtered_audio = butterworth_filter(audio)

    # Normalize the audio amplitude
    normalized_audio = normalize_audio(filtered_audio)

    # Resample the audio signal to the target length (20000 samples)
    resampled_audio = resample_audio(normalized_audio, target_length=target_length, fs=fs)

    return resampled_audio

def save_preprocessed_audio(audio, output_path):
    """
    Save the preprocessed audio signal to a .wav file.
    Args:
        audio (numpy.ndarray): Preprocessed audio signal.
        output_path (str): Path to save the audio file.
    """
    # Normalize audio to 16-bit PCM format before saving
    audio_normalized = np.int16(audio * 32767)
    write(output_path, 8000, audio_normalized)  # Save with the original sample rate

def process_dataset(dataset_folder):
    """
    Process and save the entire dataset: normalize, filter, and resample the PCG signals.
    Args:
        dataset_folder (str): Path to the dataset folder.
    """
    # List all subfolders (categories) in the dataset folder
    categories = ['AS', 'MR', 'MS', 'MVP', 'N']

    # Process each category
    for category in categories:
        print(f"Processing category: {category}")

        # List all audio files in the current category folder
        audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]

        # Process each audio file
        for audio_file in audio_files:
            # Path to the current audio file
            file_path = os.path.join(dataset_folder, category, audio_file)

            # Preprocess the audio
            preprocessed_audio = preprocess_pcg_audio(file_path)

            # Save the preprocessed audio with a new name (e.g., "processed_")
            output_path = os.path.join(dataset_folder, category, f"processed_{audio_file}")
            save_preprocessed_audio(preprocessed_audio, output_path)

    print("Processing complete.")

# Path to your dataset folder
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# Call the function to process the dataset
process_dataset(dataset_folder)

from IPython.display import Audio
import librosa.display
import matplotlib.pyplot as plt

# Define the path to the augmented audio file
# Make sure this path corresponds to where the file is stored in your Google Drive
processed_audio_file = '/content/drive/MyDrive/PHONOCARDIOGRAM/AS/augmented_New_AS_001.wav'

# Load the processed audio
processed_audio, sr = librosa.load(processed_audio_file)

# Compute the Mel spectrogram
mel_spectrogram = librosa.feature.melspectrogram(y=processed_audio, sr=sr, n_mels=128, fmax=4000)
mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)

# Plot the spectrogram
plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', cmap='viridis')
plt.colorbar(format='%+2.0f dB')
plt.title('Preprocessed Mel-frequency spectrogram')
plt.show()

# Optional: Play the audio for verification
Audio(processed_audio_file)

"""## CNN MODEL"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten

# Define the CNN model
def build_cnn_model(input_shape=(20000, 1)):
    model = models.Sequential()

    # Input layer
    model.add(layers.InputLayer(input_shape=input_shape))

    # Convolutional layers with Batch Normalization
    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=3, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(32, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2))
    model.add(Dropout(0.15))

    # Fully connected layer (Dense layer)
    model.add(Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(Dropout(0.3))

    # Output layer with 5 classes (for multi-class classification)
    model.add(layers.Dense(5, activation='softmax'))  # 5 classes

    # Model summary
    model.summary()

    return model

"""## MODEL TRAINING

"""

import os
print(os.listdir('/content/drive/MyDrive'))  # Check if 'PHONOCARDIOGRAM' folder is there
print(os.listdir('/content/drive/MyDrive/PHONOCARDIOGRAM'))  # List the categories

import numpy as np
from sklearn.model_selection import train_test_split
import librosa
import os
from tensorflow.keras.utils import to_categorical

# Load and preprocess the data
def load_data(dataset_folder, categories, sample_length=20000):
    X = []
    y = []

    for idx, category in enumerate(categories):
        print(f"Loading preprocessed data for category: {category}")
        # Modify the file listing to only include files starting with 'processed_' and 'processed_augmented_New'
        audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.startswith(('processed_augmented_New', 'processed_New')) and f.endswith('.wav')]

        if not audio_files:
            print(f"No preprocessed files found in {category}. Please ensure the preprocessing step was run correctly.")
            continue # Skip this category if no processed files are found

        for audio_file in audio_files:
            file_path = os.path.join(dataset_folder, category, audio_file)
            # Load the preprocessed audio file.
            # Since preprocessing includes resampling, the length should be close to sample_length.
            # We still load at 8000 Hz as that's the sample rate used during preprocessing.
            audio, sr = librosa.load(file_path, sr=8000)

            # Verify and potentially pad/truncate if needed (though preprocessing should handle this)
            if len(audio) < sample_length:
                audio = np.pad(audio, (0, sample_length - len(audio)), 'constant')
            else:
                 # Truncate to the exact sample length if slightly longer
                 audio = audio[:sample_length]

            X.append(audio)
            y.append(idx)

    X = np.array(X)
    y = np.array(y)

    # Reshape the data to match the CNN model input shape (samples, timesteps, channels)
    X = X.reshape((X.shape[0], sample_length, 1))

    return X, y

# Path to the dataset folder
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# List of categories (conditions)
categories = ['AS', 'MR', 'MS', 'MVP', 'N']

# Load and preprocess the data (this will now load the saved preprocessed files)
X, y = load_data(dataset_folder, categories)

# Print the shape of X and y to verify data is loaded correctly
print(f"Data shape: {X.shape}")
print(f"Labels shape: {y.shape}")

# Split the data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% temp
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split temp into val and test

# Check the shapes of the datasets
print(f"Training data shape: {X_train.shape}")
print(f"Validation data shape: {X_val.shape}")
print(f"Test data shape: {X_test.shape}")

# Convert labels to one-hot encoding for multi-class classification
y_train_one_hot = to_categorical(y_train, num_classes=5)
y_val_one_hot = to_categorical(y_val, num_classes=5)
y_test_one_hot = to_categorical(y_test, num_classes=5)

# Check the one-hot encoded labels shape
print(f"One-hot encoded training labels shape: {y_train_one_hot.shape}")
print(f"One-hot encoded validation labels shape: {y_val_one_hot.shape}")
print(f"One-hot encoded test labels shape: {y_test_one_hot.shape}")

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical

# Assuming X_train, y_train, X_val, y_val are preprocessed and ready
# Convert labels to one-hot encoding
y_train_one_hot = to_categorical(y_train, num_classes=5)
y_val_one_hot = to_categorical(y_val, num_classes=5)

# Initialize and compile the model
cnn_model = build_cnn_model(input_shape=(20000, 1))

# Compile the model using the Adam optimizer
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define EarlyStopping callback (monitoring val_loss)
early_stopping = EarlyStopping(monitor='val_loss',
                               patience=15,  # Early stop after 10 epochs of no improvement
                               restore_best_weights=True)

# Define ModelCheckpoint callback to save the best model based on val_loss
checkpoint = ModelCheckpoint('fas_mnist_1.keras',
                              monitor='val_loss',
                              save_best_only=True,
                              mode='min',
                              verbose=1)

# Train the model
history = cnn_model.fit(X_train, y_train_one_hot,
                        validation_data=(X_val, y_val_one_hot),
                        batch_size=32,
                        epochs=20,
                        callbacks=[early_stopping, checkpoint])

cnn_model.save('/content/drive/MyDrive/fas_mnist_1.keras')

"""## EVALUATION"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.metrics import (
    classification_report, accuracy_score, precision_score,
    roc_auc_score, confusion_matrix
)
from tensorflow.keras.utils import to_categorical

# Load the trained model
model = tf.keras.models.load_model('/content/drive/MyDrive/fas_mnist_1.keras')

# Predict on test set
y_pred_prob = model.predict(X_test)  # Probabilities
y_pred = np.argmax(y_pred_prob, axis=1)  # Predicted class

# Classification Report
report = classification_report(y_test, y_pred, target_names=['AS', 'MR', 'MS', 'MVP', 'N'])
print("Classification Report:\n", report)

# Accuracy and Precision
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

precision = precision_score(y_test, y_pred, average=None)
print(f"Precision (per class): {precision}")

# ROC AUC Score
y_test_one_hot = to_categorical(y_test, num_classes=5)
roc_auc_scores = roc_auc_score(y_test_one_hot, y_pred_prob, multi_class='ovr', average=None)
for i, class_name in enumerate(['AS', 'MR', 'MS', 'MVP', 'N']):
    print(f"ROC AUC for class {class_name}: {roc_auc_scores[i]:.4f}")

roc_auc_macro = roc_auc_score(y_test_one_hot, y_pred_prob, multi_class='ovr', average='macro')
print(f"Macro-average ROC AUC: {roc_auc_macro:.4f}")

# ROC Curve
plt.figure(figsize=(10, 7))
class_names = ['AS', 'MR', 'MS', 'MVP', 'N']
for i in range(5):
    fpr, tpr, _ = roc_curve(y_test_one_hot[:, i], y_pred_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], '--', color='gray')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Heart Sound Classification')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['AS', 'MR', 'MS', 'MVP', 'N'],
            yticklabels=['AS', 'MR', 'MS', 'MVP', 'N'])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

"""## COBA RUNNING CODE"""

import librosa
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
import os
from google.colab import files
from IPython.display import Audio
import librosa.display
import matplotlib.pyplot as plt

# Load the trained model
model = tf.keras.models.load_model('/content/drive/MyDrive/fas_mnist_1.keras')  # Replace with your model's path

# Define the function to preprocess the input file
def preprocess_audio(file_path, sample_length=20000):
    # Load the audio file
    audio, sr = librosa.load(file_path, sr=8000)  # Make sure to load at 8000 Hz

    # Ensure the audio signal has the right length (pad or truncate)
    if len(audio) < sample_length:
        audio = np.pad(audio, (0, sample_length - len(audio)), 'constant')  # Zero padding if shorter
    else:
        audio = audio[:sample_length]  # Truncate if longer

    # Reshape audio to match the model input shape
    audio = audio.reshape((1, sample_length, 1))  # Batch size of 1, and 1 channel
    return audio

# Function to predict the class for a new .wav file
def predict_audio(file_path):
    # Preprocess the audio file
    audio = preprocess_audio(file_path)

    # Make a prediction
    prediction = model.predict(audio)

    # Get the class with the highest probability
    predicted_class = np.argmax(prediction)

    # Define the categories (classes)
    categories = ['AS', 'MR', 'MS', 'MVP', 'N']

    # Print the prediction result
    print(f"Prediction: {categories[predicted_class]} (Class {predicted_class})")

    # Return the predicted category
    return categories[predicted_class]

# Upload the file
uploaded = files.upload()

# Get the uploaded file's name
file_name = list(uploaded.keys())[0]

# Check if the uploaded file is a .wav file
if file_name.endswith('.wav'):
    # Play the uploaded audio file
    display(Audio(file_name))  # This will embed an audio player in the notebook

    # Load the audio file again to visualize the spectrogram and frequency content
    audio_data, sr = librosa.load(file_name, sr=8000)

    # Plot the Spectrogram
    plt.figure(figsize=(10, 6))
    D = librosa.amplitude_to_db(librosa.stft(audio_data), ref=np.max)  # Short-Time Fourier Transform (STFT)
    librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr)
    plt.colorbar(format='%+2.0f dB')
    plt.title('Spectrogram')
    plt.show()

    # Plot the frequency content using Fourier Transform
    plt.figure(figsize=(10, 6))
    plt.plot(librosa.fft_frequencies(sr=sr), np.abs(librosa.stft(audio_data)).mean(axis=1))
    plt.title('Frequency Content of Audio')
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('Magnitude')
    plt.show()

    # Predict the class of the uploaded audio file
    predicted_label = predict_audio(file_name)
else:
    print("Error: The uploaded file is not a .wav file.")