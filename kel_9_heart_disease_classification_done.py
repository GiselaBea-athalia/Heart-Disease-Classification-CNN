# -*- coding: utf-8 -*-
"""KEL 9_HEART DISEASE CLASSIFICATION  DONE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pQDZjPzhjBm6TnuinjWijCaWg1eDoaM5
"""

!pip install tensorflow matplotlib

import scipy.signal as signal  # Make sure this import is included

from google.colab import drive
drive.mount('/content/drive')

import os
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt


dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'

# List of subfolder names corresponding to the 5 conditions
categories = ['AS', 'MR', 'MS', 'MVP', 'N']
def load_audio_from_drive(file_name, folder):
    file_path = os.path.join(dataset_folder, folder, file_name)
    audio, sr = librosa.load(file_path, sr=8000)
    return audio, sr

# Function to compute Mel spectrogram
def compute_spectrogram(audio, sr):
    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=4000)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
    return mel_spectrogram_db
def pad_spectrogram(spectrogram, target_width):
    pad_width = target_width - spectrogram.shape[1]
    if pad_width > 0:
        spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode='constant')
    return spectrogram
def plot_spectrogram(spectrogram):
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(spectrogram, cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Mel-frequency spectrogram')
    plt.show()
spectrograms = []
labels = []
max_width = 0

for category in categories:
    print(f"Processing category: {category}")
    audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]

    for audio_file in audio_files:
        audio, sr = load_audio_from_drive(audio_file, category)
        spectrogram = compute_spectrogram(audio, sr)
        if spectrogram.shape[1] > max_width:
            max_width = spectrogram.shape[1]
        spectrograms.append(spectrogram)
        labels.append(categories.index(category))
padded_spectrograms = [pad_spectrogram(s, max_width) for s in spectrograms]
padded_spectrograms = np.array(padded_spectrograms)
labels = np.array(labels)

print(f"Padded Spectrograms shape: {padded_spectrograms.shape}")
print(f"Labels shape: {labels.shape}")

if len(padded_spectrograms) > 0:
    plot_spectrogram(padded_spectrograms[0])
else:
    print("No spectrograms to display.")

from google.colab import drive
drive.mount('/content/drive')
import os
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'
categories = ['AS', 'MR', 'MS', 'MVP', 'N']
def load_audio_from_drive(file_name, folder):
    file_path = os.path.join(dataset_folder, folder, file_name)
    audio, sr = librosa.load(file_path, sr=8000)
    return audio, sr

def compute_spectrogram(audio, sr):
    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=4000)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
    return mel_spectrogram_db
def plot_spectrograms_for_category(category, audio_files):
    plt.figure(figsize=(15, 8))
    for i, audio_file in enumerate(audio_files[:5]):
        audio, sr = load_audio_from_drive(audio_file, category)
        spectrogram = compute_spectrogram(audio, sr)
        plt.subplot(2, 3, i+1)
        librosa.display.specshow(spectrogram, x_axis='time', y_axis='mel', cmap='viridis')
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'{category} - Sample {i+1}')

    plt.tight_layout()
    plt.show()
for category in categories:
    print(f"Processing and displaying spectrograms for category: {category}")
    audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]  # Filter for .wav files
    plot_spectrograms_for_category(category, audio_files)

dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'
categories = ['AS', 'MR', 'MS', 'MVP', 'N']
labels = ['Aortic Stenosis', 'Mitral Regurgitation', 'Mitral Stenosis', 'Mitral Valve Prolapse', 'Normal']
def plot_time_domain_signal(audio, title, sr=8000):
    time = np.linspace(0, len(audio) / sr, num=len(audio))
    plt.plot(time, audio)
    plt.title(title)
    plt.xlabel('Time [seconds]')
    plt.ylabel('Amplitude')
    plt.grid(True)
plt.figure(figsize=(15, 10))

for idx, category in enumerate(categories):
    audio_file = os.listdir(os.path.join(dataset_folder, category))[0]
    audio, sr = librosa.load(os.path.join(dataset_folder, category, audio_file), sr=8000)
    plt.subplot(2, 3, idx + 1)
    plot_time_domain_signal(audio, f'({chr(97+idx)}) {labels[idx]} PCG Signal', sr=sr)

plt.tight_layout()
plt.show()

from IPython.display import Audio
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'
categories = ['AS', 'MR', 'MS', 'MVP', 'N']
def play_audio_from_folder(folder):
    audio_files = [f for f in os.listdir(os.path.join(dataset_folder, folder)) if f.endswith('.wav')]
    if audio_files:
        file_path = os.path.join(dataset_folder, folder, audio_files[0])
        return file_path
    else:
        return None
for category in categories:
    print(f"\n\n### {category} (Condition: {category}) ###")
    audio_file_path = play_audio_from_folder(category)
    if audio_file_path:
        display(Audio(audio_file_path))
    else:
        print(f"No audio files found in the {category} folder.")

"""PCG AUGMENTATION (jangan di run lagi)"""

import numpy as np
import librosa
import librosa.display
import os
import matplotlib.pyplot as plt
from scipy.io.wavfile import write
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'
categories = ['AS', 'MR', 'MS', 'MVP', 'N']

# Background Deformation Augmentation Function
def background_deformation(audio, deformation_lambda=1000):
    """
    Apply background deformation (noise) to the audio signal.
    Args:
        audio (numpy.ndarray): Original audio signal.
        deformation_lambda (float): Control parameter for the deformation noise.
    Returns:
        numpy.ndarray: Augmented audio signal.
    """
    # Generate a random background deformation signal β
    deformation = np.random.uniform(0, 1, size=audio.shape)
    # Apply the deformation according to the formula α = I + β * λ
    augmented_audio = audio + deformation * deformation_lambda
    # Clip the values to ensure they stay within the valid range of audio signal
    augmented_audio = np.clip(augmented_audio, -1.0, 1.0)

    return augmented_audio

# Function to save augmented audio to a file
def save_augmented_audio(audio, output_path):
    """
    Save the augmented audio to a .wav file.
    Args:
        audio (numpy.ndarray): The augmented audio signal.
        output_path (str): The file path where the augmented audio will be saved.
    """
    # Normalize audio to 16-bit PCM format before saving
    audio_normalized = np.int16(audio * 32767)
    write(output_path, 8000, audio_normalized)

# Process and augment the dataset
def augment_dataset():
    augmented_data = []

    for category in categories:
        print(f"Processing and augmenting data for category: {category}")
        audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]  # Filter for .wav files

        for audio_file in audio_files:
            file_path = os.path.join(dataset_folder, category, audio_file)
            audio, sr = librosa.load(file_path, sr=8000)
            augmented_audio = background_deformation(audio, deformation_lambda=1000)
            augmented_file_path = os.path.join(dataset_folder, category, f"augmented_{audio_file}")
            save_augmented_audio(augmented_audio, augmented_file_path)
            augmented_data.append((augmented_audio, category))

    return augmented_data
augmented_data = augment_dataset()
def show_augmented_spectrogram(augmented_audio):
    mel_spectrogram = librosa.feature.melspectrogram(y=augmented_audio, sr=8000, n_mels=128, fmax=4000)
    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', cmap='viridis')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Augmented Mel-frequency spectrogram')
    plt.show()
if augmented_data:
    show_augmented_spectrogram(augmented_data[0][0])

"""PREPROCESSING (jangan di run lagi)

"""

def normalize_audio(audio):
    """
    Normalize the audio signal to fit within the 16-bit range.
    Args:
        audio (numpy.ndarray): Audio signal.
    Returns:
        numpy.ndarray: Normalized audio signal.
    """
    # Normalize to the range [-1, 1]
    audio = audio / np.max(np.abs(audio))

    # Convert to floating-point representation (16-bit normalized audio should be in range [-1, 1])
    audio_normalized = audio.astype(np.float32)
    return audio_normalized

def butterworth_filter(audio, lowcut=20.0, highcut=150.0, fs=8000, order=4):
    """
    Apply a Gaussian Butterworth filter to the audio signal to remove high-frequency noise.
    Args:
        audio (numpy.ndarray): Audio signal.
        lowcut (float): Low cutoff frequency in Hz (default 20 Hz).
        highcut (float): High cutoff frequency in Hz (default 150 Hz).
        fs (int): Sample rate (default 8000 Hz).
        order (int): Order of the filter (default 4).
    Returns:
        numpy.ndarray: Filtered audio signal.
    """
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist

    # Create Butterworth filter
    b, a = signal.butter(order, [low, high], btype='band')
    filtered_audio = signal.filtfilt(b, a, audio.astype(np.float32))  # Ensure it's float32

    return filtered_audio

def resample_audio(audio, target_length=20000, fs=8000):
    """
    Resample the audio signal to a target length (20,000 samples) at a new sample rate.
    Args:
        audio (numpy.ndarray): Audio signal.
        target_length (int): Target length for the audio in samples (default 20,000).
        fs (int): Sample rate of the original signal (default 8000 Hz).
    Returns:
        numpy.ndarray: Resampled audio signal.
    """
    # Resample the audio signal to match the target length
    duration = len(audio) / fs
    target_fs = target_length / duration  # Target sample rate
    resampled_audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)
    if len(resampled_audio) < target_length:
        resampled_audio = np.pad(resampled_audio, (0, target_length - len(resampled_audio)), 'constant')
    else:
        resampled_audio = resampled_audio[:target_length]

    return resampled_audio

def preprocess_pcg_audio(file_path, fs=8000, target_length=20000):
    """
    Preprocess the PCG audio signal: normalize, filter, and resample.
    Args:
        file_path (str): Path to the audio file.
        fs (int): Sample rate of the audio signal (default 8000 Hz).
        target_length (int): Target length for resampling (default 20,000).
    Returns:
        numpy.ndarray: Preprocessed audio signal.
    """
    audio, _ = librosa.load(file_path, sr=fs)
    filtered_audio = butterworth_filter(audio)
    normalized_audio = normalize_audio(filtered_audio)
    resampled_audio = resample_audio(normalized_audio, target_length=target_length, fs=fs)
    return resampled_audio

def save_preprocessed_audio(audio, output_path):
    """
    Save the preprocessed audio signal to a .wav file.
    Args:
        audio (numpy.ndarray): Preprocessed audio signal.
        output_path (str): Path to save the audio file.
    """
    audio_normalized = np.int16(audio * 32767)
    write(output_path, 8000, audio_normalized)

def process_dataset(dataset_folder):
    """
    Process and save the entire dataset: normalize, filter, and resample the PCG signals.
    Args:
        dataset_folder (str): Path to the dataset folder.
    """
    categories = ['AS', 'MR', 'MS', 'MVP', 'N']
    for category in categories:
        print(f"Processing category: {category}")
        audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]
        for audio_file in audio_files:
            file_path = os.path.join(dataset_folder, category, audio_file)
            preprocessed_audio = preprocess_pcg_audio(file_path)
            output_path = os.path.join(dataset_folder, category, f"processed_{audio_file}")
            save_preprocessed_audio(preprocessed_audio, output_path)
    print("Processing complete.")

dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'
process_dataset(dataset_folder)

from IPython.display import Audio
import librosa.display
import matplotlib.pyplot as plt
processed_audio_file = '/content/drive/MyDrive/PHONOCARDIOGRAM/AS/augmented_New_AS_001.wav'
processed_audio, sr = librosa.load(processed_audio_file)
mel_spectrogram = librosa.feature.melspectrogram(y=processed_audio, sr=sr, n_mels=128, fmax=4000)
mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)
plt.figure(figsize=(10, 4))
librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', cmap='viridis')
plt.colorbar(format='%+2.0f dB')
plt.title('Augmented Mel-frequency spectrogram')
plt.show()
Audio(processed_audio_file)

"""CNN MODEL"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten

# Define the CNN model
def build_cnn_model(input_shape=(20000, 1)):
    model = models.Sequential()

    # Input layer
    model.add(layers.InputLayer(input_shape=input_shape))

    # Convolutional layers with Batch Normalization
    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(64, 3, strides=3, padding='valid', activation='relu'))
    model.add(BatchNormalization())

    model.add(layers.Conv1D(32, 3, strides=2, padding='valid', activation='relu'))
    model.add(BatchNormalization())
    model.add(layers.MaxPooling1D(pool_size=2))
    model.add(Dropout(0.15))

    # Fully connected layer (Dense layer)
    model.add(Flatten())
    model.add(layers.Dense(128, activation='relu'))
    model.add(Dropout(0.3))

    # Output layer with 5 classes (for multi-class classification)
    model.add(layers.Dense(5, activation='softmax'))  # 5 classes

    # Model summary
    model.summary()

    return model

"""MODEL TRAINING

"""

import os
print(os.listdir('/content/drive/MyDrive'))
print(os.listdir('/content/drive/MyDrive/PHONOCARDIOGRAM'))

import numpy as np
from sklearn.model_selection import train_test_split
import librosa
import os
from tensorflow.keras.utils import to_categorical
def load_data(dataset_folder, categories, sample_length=20000):
    X = []
    y = []

    for idx, category in enumerate(categories):
        audio_files = [f for f in os.listdir(os.path.join(dataset_folder, category)) if f.endswith('.wav')]
        for audio_file in audio_files:
            file_path = os.path.join(dataset_folder, category, audio_file)
            audio, sr = librosa.load(file_path, sr=8000)
            if len(audio) < sample_length:
                audio = np.pad(audio, (0, sample_length - len(audio)), 'constant')
            else:
                audio = audio[:sample_length]
            X.append(audio)
            y.append(idx)
    X = np.array(X)
    y = np.array(y)
    X = X.reshape((X.shape[0], sample_length, 1))

    return X, y
dataset_folder = '/content/drive/MyDrive/PHONOCARDIOGRAM/'
categories = ['AS', 'MR', 'MS', 'MVP', 'N']
X, y = load_data(dataset_folder, categories)
print(f"Data shape: {X.shape}")
print(f"Labels shape: {y.shape}")
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% temp
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split temp into val and test

print(f"Training data shape: {X_train.shape}")
print(f"Validation data shape: {X_val.shape}")
print(f"Test data shape: {X_test.shape}")
y_train_one_hot = to_categorical(y_train, num_classes=5)
y_val_one_hot = to_categorical(y_val, num_classes=5)
y_test_one_hot = to_categorical(y_test, num_classes=5)
print(f"One-hot encoded training labels shape: {y_train_one_hot.shape}")
print(f"One-hot encoded validation labels shape: {y_val_one_hot.shape}")
print(f"One-hot encoded test labels shape: {y_test_one_hot.shape}")

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical
y_train_one_hot = to_categorical(y_train, num_classes=5)
y_val_one_hot = to_categorical(y_val, num_classes=5)
cnn_model = build_cnn_model(input_shape=(20000, 1))
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss',
                               patience=100,
                               restore_best_weights=True)
checkpoint = ModelCheckpoint('fas_mnist_1.keras',
                              monitor='val_loss',
                              save_best_only=True,
                              mode='min',
                              verbose=1)

history = cnn_model.fit(X_train, y_train_one_hot,
                        validation_data=(X_val, y_val_one_hot),
                        batch_size=32,
                        epochs=50,
                        callbacks=[early_stopping, checkpoint])

cnn_model.save('/content/drive/MyDrive/fas_mnist_1.keras')

"""Coba code"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, precision_score, accuracy_score, confusion_matrix, classification_report
import tensorflow as tf
import numpy as np
y_pred_prob = cnn_model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)

report = classification_report(y_test, y_pred, target_names=['AS', 'MR', 'MS', 'MVP', 'N'])
print("Classification Report:\n", report)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
precision = precision_score(y_test, y_pred, average=None)
print(f"Precision (per class): {precision}")
y_test_one_hot = to_categorical(y_test, num_classes=5)
roc_auc = roc_auc_score(y_test_one_hot, y_pred_prob, multi_class='ovr')
print(f"ROC AUC: {roc_auc:.4f}")
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AS', 'MR', 'MS', 'MVP', 'N'], yticklabels=['AS', 'MR', 'MS', 'MVP', 'N'])
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

import librosa
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
import os
from google.colab import files
from IPython.display import Audio
import librosa.display
import matplotlib.pyplot as plt

model = tf.keras.models.load_model('/content/drive/MyDrive/fas_mnist_1.keras')
def preprocess_audio(file_path, sample_length=20000):
    audio, sr = librosa.load(file_path, sr=8000)

    if len(audio) < sample_length:
        audio = np.pad(audio, (0, sample_length - len(audio)), 'constant')
    else:
        audio = audio[:sample_length]
    audio = audio.reshape((1, sample_length, 1))
    return audio
def predict_audio(file_path):
    audio = preprocess_audio(file_path)
    prediction = model.predict(audio)
    predicted_class = np.argmax(prediction)
    categories = ['AS', 'MR', 'MS', 'MVP', 'N']
    print(f"Prediction: {categories[predicted_class]} (Class {predicted_class})")
    return categories[predicted_class]
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
if file_name.endswith('.wav'):
    display(Audio(file_name))
    audio_data, sr = librosa.load(file_name, sr=8000)

    # Plot the Spectrogram
    plt.figure(figsize=(10, 6))
    D = librosa.amplitude_to_db(librosa.stft(audio_data), ref=np.max)  # Short-Time Fourier Transform (STFT)
    librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr)
    plt.colorbar(format='%+2.0f dB')
    plt.title('Spectrogram')
    plt.show()
    plt.figure(figsize=(10, 6))
    plt.plot(librosa.fft_frequencies(sr=sr), np.abs(librosa.stft(audio_data)).mean(axis=1))
    plt.title('Frequency Content of Audio')
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('Magnitude')
    plt.show()
    predicted_label = predict_audio(file_name)
else:
    print("Error: The uploaded file is not a .wav file.")